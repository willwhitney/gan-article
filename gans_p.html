<h1 id="an-introduction-to-generative-adversarial-nets">An introduction to Generative Adversarial Nets</h1>
<p>Generative Adversarial Networks (GANs) have captured the imagination of the deep learning community in the past year, and they have come to dominate the world of generative image modeling to an impressive degree. While we still struggle to generate images that are really plausible, for the first time we can produce big, sharp pictures of hundreds of different objects without building a renderer by hand.</p>
<p>Here are a few examples from a GAN trained on 128x128 images from the ImageNet dataset<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>:</p>
<p><a href="https://openai.com/blog/generative-models/"><img src="openai-imagenet.jpg" alt="Imagenet generated samples" /></a></p>
<p>Though none of these images quite manage to show real objects, their clarity and overall first-glance recognizability is absolutely stunning compared to similar results from not long ago.</p>
<p>Wow factor aside, what's going on under the hood? Why is it hard to generate an image? And why are GANs so good at it?</p>
<h2 id="the-problem-of-generative-modeling">The problem of generative modeling</h2>
<p>Researchers have been building generative models from raw data for years. The idea is that if you look at enough photos, for example, you should be able to create an image of your own that looks like a new photograph. But this is tricky, because how do you measure the quality of the new picture you generate?</p>
<p>One straightforward metric for an image's quality is the <strong>distance to its nearest neighbor.</strong> Any image you generate should probably be &quot;close&quot; to some image in the dataset. If you had lots and lots of pictures of elephants in your dataset, any new image of an elephant that you create is likely to be broadly similar and have the same color palette, etc.</p>
<p>In principle, with an infinite number of example images, this is a great start at choosing an objective function! Imagine that you had every <em>possible</em> image of an elephant in your <del>pocket universe</del> hard drive. Each time you you generate a correct image of an elephant, it's already on your hard drive! If it's not, your new image is not a good elephant photo, in which case it makes sense to see which elephant photo it's closest to, then make your photo more like that photo.</p>
<p>This update procedure, making your generated image <span class="math inline">\(\hat{x}\)</span> more similar to the most-similar image from the dataset, corresponds to minimizing the loss function <span class="math display">\[
\mathcal{L}(\hat{x}) = \min_{x \in data} (x - \hat{x})^2
\]</span> This equation says that the &quot;error&quot; for some image <span class="math inline">\(\hat{x}\)</span> that you create is simply how far it is from the most similar image in the dataset.</p>
<p>What if you only have a finite number of images?</p>
<!-- Perhaps the set of all good elephant pictures forms a smooth manifold. That is, nearby to each elephant picture, there are a lot of other elephant pictures linearly related to it, so that if you made the picture one tiny $\epsilon$ brighter or dimmer, or more red or blue, you would find another elephant picture. -->
<p>Think of your infinite elephant pictures as a plastic tarp, where every spot on the tarp corresponds to one photo in your infinite set. But if instead we have only <span class="math inline">\(10^{100}\)</span> images, it's more like a 5000 thread count silk sheet. It's still very smooth, and you definitely can't see through it, but in principle something very thin in just the right spot could go through without breaking any threads. There are at least <em>some</em> elephant photos that aren't in your dataset.</p>
<p>Now what if you only have, say, one ImageNet of images: about a million. Even though a million images seems like a lot, they're spread across the space of <em>all possible natural images</em>. The dog in that photo <em>could</em> roll a tiny bit to the left, and that photo isn't in your dataset. Once the light turns green in that intersection, that scene is no longer in your dataset. And neither is every instant after that! With only a million images, your fine silk sheets are more like a fishing net; they pretty much cover the whole area, but about 0% of allowable images are in your data.</p>
<p>So with a finite dataset, your model will always be spending a certain amount of effort over-optimizing. Maybe what it produced was a perfectly reasonable elephant picture, but since it wasn't in the dataset, the model tries to make it more like whatever random image happened to make it into the data. This can result in overfitting and also just inefficient learning.</p>
<h2 id="encouraging-variety">Encouraging variety</h2>
<p>But there's an even worse caveat. The loss function we defined above trains your model to to <em>always</em> produce images that are in the dataset, but does nothing to encourage it to produce <em>all</em> the images in the dataset! Or, to put it another way, your model could have perfect <em>precision</em>, but terrible <em>recall</em>.</p>
<p>In the simplest case, your model could learn to always produce <em>the same image</em>. That is, every time you sample from your model, it gives you back the same image of an elephant, exactly copied from the dataset. But! You're a machine learning expert! You know the solution to this problem. You want to encourage diversity in your samples, so you add a regularization term to your model which penalizes it for generating which are too close together.</p>
<p>In practice this is quite difficult to get right. To exactly calculate this regularization term, you must compute the distance between every pair of images you create. Imagine that each image you generate is a ball, and between each pair of balls there is a spring pushing them away from one another. The closer two balls are together, the more they get pushed apart by their spring. If you have only a few image samples, this is relatively cheap to compute, but won't give a very accurate calculation of the regularization. And with more samples, the cost goes as <span class="math inline">\(O(n^2)\)</span>, and will rapidly dominate your training cost.</p>
<p>If only we had some sort of universal function approximator that we could use to amortize the cost of such a calculation...</p>
<h2 id="the-gan-idea">The GAN idea</h2>
<p>Every few years I make a desultory attempt to learn how to draw. I never get very far, and each time I'm struck by the same thought: I have <em>no idea</em> what a tree looks like. This is a really strange realization for someone who grew up in the suburbs.</p>
<p>Obviously it's not exactly true, either. If you showed me several photos, I would be able to tell you which ones have trees in them. I might even be able to say which of several quite good sketches was more realistic. But when the sketches are as bad as mine, I find I have no idea what I need to change to make them better. <a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>In some sense this is the problem GANs address.</p>
<h3 id="model">Model</h3>
<p>A GAN consists of two components: a Generator and a discriminator (or Adversary). The generator is a function which maps random noise to images (or some other kind of data), and the discriminator is a function which maps images to a prediction of whether they're real (i.e. drawn from the dataset) or not.</p>
<p>During training, you sample random noise, then put it through the generator to produce something (hopefully!) image-like. Then you take that image and put it through the discriminator. Since you know that was a fake image, you can tell the discriminator that it was a negative (fake) example. Then you backpropagate through the discriminator to make it more likely to classify that image as fake.</p>
<p>At the end of backpropagation through the discriminator, you get gradients on the input to the discriminator which indicate which way you could perturb that input to make the discriminator more likely to classify it correctly. But now notice two facts:</p>
<ol style="list-style-type: decimal">
<li>The input to the discriminator is the output of the generator.</li>
<li>We can do a backward pass through the discriminator with the <em>wrong target</em> to find out what we could change in the image to make the discriminator classify it as a real image.</li>
</ol>
<!-- The gradient of incorrect classification with respect to the image (i.e. how to change the photo to fool the discriminator) is the negative of the gradient of correct classification, which backprop already gave us. -->
<p>We can use the gradients from the discriminator to train the generator! These gradients tell the generator exactly what it needs to change about this image in order to make it better (that is, to make it more likely to fool the discriminator). Since we're using the discriminator to tell the generator how well it's doing, we call this a <em>discriminitive loss function</em> for the generator.</p>
<p>This clever use of a differentiable discriminitive model to evaluate the generated image is what makes the GAN a better artist than I am.</p>
<p>For the rest of this article, I'll be mostly talking about GANs as a way to train a good generator function. In this setting, the thing you want is a generative model for images, and after training the generator you will discard the discriminator.</p>
<h2 id="why-do-gans-work-so-well">Why do GANs work so well?</h2>
<p>There are a lot of differences between a GAN and something more traditional like an autoencoder, so it's worth briefly discussing them before we delve into the effects each one has.</p>
<h3 id="priors">Priors</h3>
<p>For one thing, the GAN is a generative model with a well-defined prior.</p>
<p>By contrast, imagine that you have only the generating half of an autoencoder and you want to generate an image similar to one in the training set. What input do you give that generator?</p>
<div class="figure">
<img src="autoencoder.svg" alt="An autoencoder." />
<p class="caption">An autoencoder.</p>
</div>
<p>The autoencoder training objective is <span class="math inline">\(g(f(x)) \approx x\)</span>, so the domain of <span class="math inline">\(g\)</span> is the range of <span class="math inline">\(f\)</span>. Intuitively, the only inputs that <span class="math inline">\(g\)</span> will be trained to deal with are those that <span class="math inline">\(f\)</span> actually produces over your dataset.</p>
<p>Naively, it could be that just about anything you give the decoder will produce a good image. One way to think about an autoencoder is that it compresses its input, and if it has optimally solved this task, any input in its domain should map to a valid output. That is to say, an optimal code has no unused bitstrings.</p>
<!-- say more clearly what I mean by not filling its input space -->
<p>In practice, though, there doesn't seem to be much pressure for the autoencoder to fill its input space. Instead, a typical encoder will have some much smaller range embedded in the space of all the codes it could produce. As a result, most inputs you feed into the decoder won't generate anything like a real image. And it's not so simple as picking inputs in some particular range, which would correspond to just taking a rectangular slice of the space. That decoder's true domain could be <em>crazy</em>.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>Imagine you have a decoder that takes two real numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> as inputs and generates images (or at least, image-shaped things). You might hope that if each input is between -1 and 1, like the blue square below, the decoder will produce a real image as its output. However, the decoder's real domain may be a crazy-looking manifold like the green curve, where only some combinations of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> will produce good-looking images.</p>
<div class="figure">
<img src="decoder-domain.svg" alt="Domain of decoder versus a simple guess." />
<p class="caption">Domain of decoder versus a simple guess.</p>
</div>
<p>With a GAN, sampling is baked right in. If the random noise you're using to feed the GAN is, say, <span class="math inline">\(\mathcal{N}(0,1)\)</span>, likely samples from that distribution should produce likely-looking images, and unlikely samples will make somewhat weirder images. Conveniently, that prior doesn't have any holes; if <span class="math inline">\(a\)</span> is a point in the domain of the GAN, and <span class="math inline">\(a + b\)</span> is also in the domain, then <span class="math inline">\(a + \frac{1}{2} b\)</span> is too. That's what lets people make those sweet face-arithmetic equations like in the DCGAN paper<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>:</p>
<div class="figure">
<img src="face_arithmetic.png" />

</div>
<h3 id="multimodal-loss-function">Multimodal loss function</h3>
<p>Another powerful feature of the GAN is that the discriminator provides a <em>multimodal</em> loss function. In other words, the discriminator loss function thinks there are very many good images, whereas an autoencoder's mean-squared error is asking how far the produced image is from one image in particular.</p>
<div class="figure">
<img src="loss-functions.svg" />

</div>
<p>This is a great property! Pixel-wise error is a terrible loss function and everyone knows it, but up until recently there haven't been a lot of alternatives. If the decoder of an autoencoder produces exactly the input image, but shifted left by one pixel, it may get a very large error. A GAN's discriminator, on the other hand, evaluates how &quot;credible&quot; it is that a given image appears in the dataset. That one-pixel-misaligned image will score very well.</p>
<p>The GAN discriminator loss also begins to solve a problem that we talked about earlier. If the loss function were merely evaluating how far an image is from the closest photo in the dataset, we might get a generator that always produces the same image for every input. With a GAN, though, the discriminator is trained together with the generator. As the generator learns to produce some single image, the discriminator learns that the generator always makes this one image, and so when it sees that image, it's probably a fake.</p>
<p>The optimal discriminator function<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> is <span class="math display">\[
D^*_G(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}
\]</span> <span class="math inline">\(p_{data}\)</span> is the likelihood of an image under the dataset, and <span class="math inline">\(p_g\)</span> is the likelihood of the image under the generator. Remember that since this is a probability distribution, the sum over all images <span class="math inline">\(x\)</span> of <span class="math inline">\(p_{data}(x)\)</span> is <span class="math inline">\(1\)</span>. For example, if this was a discrete distribution, we would have <span class="math display">\[
p_{data}(x) = \frac{\text{number of times }x\text{ appears in the data}}{\text{total number of data}}
\]</span> Intuitively, the optimal discriminator should say that an image is real (produce a <span class="math inline">\(1\)</span>) if it's very likely under the dataset and there's only a small chance of the generator making that image. Conversely, if it's very likely that the generator would produce such an image, and/or it doesn't seem like it would be in the dataset, the discriminator should produce a <span class="math inline">\(0\)</span>.</p>
<p>All this means that if the discriminator figures out that <span class="math inline">\(p_g(x)\)</span> is nearly 1, where the generator always produces the same image, it's going to give that image a very low score. To minimize <span class="math inline">\(p_g(x)\)</span>, the generator has to make all the images it produces as different from one another as possible. This pushes the generator to make a lot of unrelated-seeming images while ensuring that each image <span class="math inline">\(x\)</span> is similar enough to the dataset that <span class="math inline">\(p_{data}(x)\)</span> is pretty high.</p>
<p>Earlier we thought about trying to explicitly encourage samples from a generator to be far from one another in pixel space, but realized that it would be computationally intractible. In a sense the discriminator's implicit estimation of <span class="math inline">\(p_g(x)\)</span> is doing the same thing in a more principled way; it encourages the generator to spread out the images it produces as much as possible in the discriminator's feature space.</p>
<p>This is exactly what we were looking for! The discriminator loss function forces the generator to produce the <em>whole</em> dataset, or as much of it as possible, instead of just requiring it to produce <em>only</em> images from the dataset.</p>
<p>Or at least, that's the theory. In practice GANs often experience a phenomenon known as &quot;collapse&quot;, in which the generator will produce only a few really different-looking kinds of images. This is possible because the discriminator isn't perfect, and it can't recognize <em>all</em> the images that the generator produces.</p>
<p>For example, if you train a GAN on pictures of a hundred different breeds of dogs, you might get a generator which only makes images of corgis, German shepherds, and golden retrievers. Especially if those breeds were the most common in the dataset, the discriminator may not be very confident that those examples are fake; it only has the power to &quot;remember&quot; a certain number of examples in its weights.</p>
<p>Preventing collapse and encouraging diverse images is an open problem. While there have been strong efforts in this direction (notably from OpenAI<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>), nothing so far has been definitive.</p>
<h3 id="training-curriculum">Training curriculum</h3>
<p>By the end of training a GAN discriminator is able to do a pretty compelling job of evaluating how realistic an image is. In fact, a common problem in GAN training is that the discriminator may get too good at identifying the faked images. At that point, it's very difficult for the generator to learn; if everything it does is easily identified as a fake, there's no small change it can make to the image that will really improve its odds of tricking the discriminator.</p>
<p>It comes back to the conundrum of the beginner artist. If I'm so bad at drawing that my picture looks <em>nothing</em> like a tree, it's pretty hard to say which one thing I should change.</p>
<p>With careful training, though, this won't happen. Instead, the generator and the discriminator will improve in sync, gradually producing images that are more like the dataset and discerning more detailed differences between real and fake images. In this scenario, the discriminator is not just one loss function for the generator. It is a series of slowly-evolving loss functions which converge toward pixel-perfect modeling of the data distribution.</p>
<p>The dynamics of this evolution would be <em>very</em> interesting to study. I'm reminded of papers by David Duvenaud and Dougal Maclaurin<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> as well as by Andrew Saxe<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>. I would love to see similar ideas used to analyze the curriculum induced by GAN training.</p>
<!-- The startling thing here is that somehow the order that the discriminator _happens_ to learn in creates a great curriculum for the generator. -->
<!-- In one sense, that's to be expected. After all, the discriminator is trained online against the generator. You can imagine that at each point, the discriminator finds a characteristic that is in some sense "most wrong" in the generated images, then penalized the generator for that characteristic. -->
<!-- But to make that statement more precise, we need to have a definition of what that "most wrong" thing is. While I -->
<h2 id="lots-of-questions-left-to-answer">Lots of questions left to answer</h2>
<p>GANs are now state of the art for image modeling, and I'm sure there are countless papers applying them to different domains under development right now. Even so, there are a few big questions that are still open.</p>
<p>To me, the most pressing question is how to encourage the generator to produce diverse samples. Especially in conditional GANs, the tendency to collapse can be extremely strong. The discriminator would have to memorize a very large set of outputs to identify generated images, and in many settings it's difficult to produce diverse batches of correct samples to strengthen the discriminator.</p>
<p>Another open area is applying GANs to discrete domains. In a continuous domain like images, the generator can make a pixel just <em>slightly</em> more red or <em>slightly</em> less green to make the image as a whole more credible to the discriminator. By contrast, it's not clear how to make a generator for sentences produce a phrase slightly closer to &quot;my fluffy pet kitty&quot; than &quot;my fluffy pet shark&quot;. While work has been done on adversarially generating language <em>embeddings</em> instead of directly generating words<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a>, the challenge of adversarial training for discrete domains is still unanswered.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p>
<p>This is one of the fastest-moving areas of computer science and I'm very excited to see what comes out in the next year. With the number of great people working on generative modeling today we may not have to wait long for solutions.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Salimans, Tim, et al. &quot;Improved Techniques for Training GANs.&quot; <em>arXiv preprint arXiv:1606.03498</em>(2016).<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>I think this is what people mean when they say that someone has an artist's eye. Seeing the image your eye captures is a really different skill from just recognizing what's going on in a scene or where things are. People get so incredibly good at turning pixels into 3D models that we don't even know what the pixels look like!<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Incidentally, this is a big part of the reason variational autoencoders are interesting. The prior of a variational autoencoder ensures that the range of the encoder, and hence the domain of the decoder, is fit to some distribution. That makes variational autoencoders very easy to sample from!<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Radford, Alec, Luke Metz, and Soumith Chintala. &quot;Unsupervised representation learning with deep convolutional generative adversarial networks.&quot;<em>arXiv preprint arXiv:1511.06434</em> (2015).<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Goodfellow, Ian, et al. &quot;Generative adversarial nets.&quot; <em>Advances in Neural Information Processing Systems</em>. 2014.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Salimans, Tim, et al. &quot;Improved Techniques for Training GANs.&quot; <em>arXiv preprint arXiv:1606.03498</em>(2016).<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Duvenaud, David, Dougal Maclaurin, and Ryan P. Adams. &quot;Early Stopping as Nonparametric Variational Inference.&quot; <em>Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</em>. 2016.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Saxe, Andrew M., James L. McClelland, and Surya Ganguli. &quot;Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.&quot; <em>arXiv preprint arXiv:1312.6120</em> (2013).<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Miyato, Takeru, Andrew M. Dai, and Ian Goodfellow. &quot;Virtual Adversarial Training for Semi-Supervised Text Classification.&quot; <em>arXiv preprint arXiv:1605.07725</em> (2016).<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>While it's possible that GANs may never be completely adapted to discrete domains, it certainly feels like it should be possible to have the generator and discriminator operate in probability space.<a href="#fnref10">↩</a></p></li>
</ol>
</div>
